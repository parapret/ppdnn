<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Parallelizing Pre-Training of Deep Neural Networks using Stacked Autoencoders</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1>Arun Sai Krishnan &amp Prakruthi Prabhakar</h1>
					<p>15-618 Final Project</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#ps" class="active">Problem Statement</a></li>
						<li><a href="#mp" class="active">Model Parallel Approach</a></li>
						<li><a href="#dp" class="active">Data Parallel Approach</a></li>
						<li><a href="#con" class="active">Conclusion</a></li>
						<li><a href="#cp">Checkpoint</a></li>
						<li><a href="#two">Background</a></li>
						<li><a href="#three">The Challenge</a></li>
						<li><a href="#four">Resources</a></li>
						<li><a href="#five">Goals &amp Deliverables</a></li>
						<li><a href="#six">Platform</a></li>
						<li><a href="#seven">Updated Schedule</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="#" class="icon fa-github"><span class="label">Github</span></a></li>
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="ps">
								<div class="container">
									<header class="major">
										<h3>Parallelizing Pretraining of Deep Neural Networks using Stacked Autoencoders</h3>
									</header>
									<h3>What is pretraining a deep neural network?</h3>
									<p>
										Given a deep neural network, pretraining is a process where the weights of the network are learnt to provide a good starting point for training the network further. It helps pre-condition the network with a set of parameter values which effectively facilitates further supervised training.
										<img src="images/glp1.png" alt="" style="width:20%;height:50%"/>
										<img src="images/glp2.png" alt="" style="width:77%;height:100%"/>
										The traditional approach to pretraining the network uses greedy layer-wise pretraining. Figure 1 illustrates a deep neural network with 3 hidden layers. The greedy layer-wise pre-training works bottom-up in a deep neural network. The algorithm begins by training the first hidden layer using an autoencoder network minimizing the reconstruction error of the input. Once this layer has been trained, its parameters are fixed and the next layer is trained in a similar manner. The process is repeated until all the layers have been trained. This process is illustrated in Figure 2. The iterative nature of this process makes pre-training a relatively slow and time consuming process.
										In this project, we explore parallelizing this process in two ways - <bold> Model Parallel </bold> approach and <bold> Data Parallel </bold> approach.
									</p>
								</div>
							</section>

							<section id="mp">
								<div class="container">
									<h3>Model Parallel Approach</h3>
									<p>
										In model parallel approach, we train each layer of the network on different nodes. Recent research has shown that a <a href="https://arxiv.org/pdf/1603.02836.pdf" target="_blank">synchronized layer-wise pre-training approach</a> using multiple cores is effective in speeding up the pre-training process by over 25 percent achieving similar accuracies on classification task.
										<img src="images/mp.png" alt="" style="width:80%;height:50%"/>
										The above figure shows the overview of model parallel pretraining. The process begins by training the first layer using an autoencoder network for one epoch. This considers all the images in the training dataset batch-wise and learns weights for the first epoch. The learned weights are then sent to the next node, which uses this as fixed and starts learning the first epoch of the second layer. The first node continues training the first layer's second epoch in parallel. This process is continued such that all layers are trained for a certain number of epochs in parallel with communication of weights between layers every epoch.
									</p>
								</div>
							</section>

						<!-- CP -->
							<section id="cp">
								<div class="container">
									<h3>Checkpoint</h3>

									<h4>Updated schedule</h4>
									<p><a href="#seven">Click here.</a></p>

									<h4>Summary of work completed so far</h4>
									<p>We have completed the following tasks so far:
										<ul>
											<li>Extensive Literature Review</li>
											<li>Setting up Tensorflow on GHC machine in the user space, attempting setup of Tensorflow on Latedays Cluster and getting familiar with Tensorflow</li>
											<li>Implement single layer pre-training using an autoencoder in Tensorflow</li>
											<li>Implement greedy layer-wise pre-training of stacked autoencoder in Tensorflow</li>
											<li>Implement single-GPU single-node synchronized layer-wise pre-training of deep stacked autoencoders</li>
											<li>Familiarizing with Multi-GPU implementations on Distributed Tensorflow</li>
											<li>Familiarizing with Socket Programming in python for communication in a distributed setting</li>
											<li>Implementing socket communication of string in between epochs of training</li>
											<li>Implementing two-GPU single-node synchronized layer-wise pre-training of deep stacked autoencoders (in debugging phase)</li>
											<li>Discussions with Anant and Ravi from course staff about progress and further ideas</li>
										</ul>
									</p>

									<h4>Progress on goals and deliverables</h4>
									<p>We planned to achieve an efficient parallel and distributed implementation of synchronized layer-wise pre-training algorithm of deep neural networks. We have experimented with greedy layer-wise pre-training of Deep Stacked Autoencoders on MNIST dataset. We have also implemented a single-GPU single-node synchronized layer-wise pre-training of deep stacked autoencoders as a baseline. We have compared the losses and accuracy values of our implementation with greedy layer-wise pretraining on MNIST dataset as done in the reference paper. This serves as a proof of concept for the algorithm as well as a baseline to test and analyse our speed up on. At this point in time, the goal we plan to achieve is an efficent multi-GPU version of this algorithm. We have implemented two-GPU single node version and we are debugging the correctness of our implementation.<br>
									We had hoped to achieve an efficient custom parameter server implementation and a generalized distributed version of this algorithm. However, on understanding the problem deeper and considering feasibilty, we concluded that the distributed implementation of this algorithm using a custom parameter server will be an overkill for the problem. An efficient multi-GPU implementation with extensive analysis on various possible placement of pretraining layers and communication strategies would be good end goal for our project. We also hope to implement a distributed GPU implementation for further analysis.</p>

									<h4>Plan for the parallelism competition</h4>
									<p>We plan to present a poster at the parallelism competition. The poster will include a detailed analysis of the results of our work and our experiments.</p>

									<h4>Challenges so far</h4>
									<p><ul>
									<li>Getting started with Tensorflow (which was new to both of us)</li>
									<li>Understanding model parallelism strategies in Tensorflow and getting familiar with multi-GPU implementation of the same</li>
									<li>Understanding various possible ops placement on devices, interrupting the model in between sessions, communication via sockets, etc.</li>
									</ul></p>

								</div>
							</section>

						<!-- Two -->
							<section id="two">
								<div class="container">
									<h3>Background</h3>
									<p>Pre-training a deep neural network helps pre-condition the network with a set of parameter values which effectively facilitates further supervised training. Greedy layer-wise pre-training using stacked autoencoders is widely used in deep learning applications. Recent research has shown that a <a href="https://arxiv.org/pdf/1603.02836.pdf" target="_blank">synchronized layer-wise pre-training approach</a> using multiple cores is effective in speeding up the pre-training process by over 25 percent. In this project, we plan to give an optimized parallel and distributed implementation of this process in a multi-core heterogenous (CPU-GPU) environment.</p>
									<p>Figure 1 illustrates a deep neural network with 3 hidden layers. The greedy layer-wise pre-training works bottom-up in a deep neural network. The algorithm begins by training the first hidden layer using an autoencoder network minimizing the reconstruction error of the input. Once this layer has been trained, its parameters are fixed and the next layer is trained in a similar manner. The process is repeated until all the layers have been trained. This process is illustrated in Figure 2. The iterative nature of this process makes pre-training a relatively slow and time consuming process.</p>
									<img src="images/glp1.png" alt="" style="width:20%;height:50%"/>
									<img src="images/glp2.png" alt="" style="width:77%;height:100%"/>
									<p><br>As mentioned earlier, <a href="https://arxiv.org/pdf/1603.02836.pdf" target="_blank">recent research</a> has shown an alternative parallel approach to this process where different layers are trained in parallel with empirically observed convergence and accuracy similar to the traditional approach. In this algorithm, different layers are trained concurrently with regular synchronization of intermediate layer updates. This enables faster pre-training. The goal of our project is to provide an efficient implementation of the same in a distributed heterogenous processor environment.</p>
								</div>
							</section>

						<!-- Three -->
							<section id="three">
								<div class="container">
									<h3>Challenges</h3>
									<p><ul>
											<li>Implementing the algorithm in a distributed setting might require an implementation of a custom parameter server.</li>
											<li>Convergence of the synchronized layer-wise pre-training algorithm might take longer than the traditional approach.</li>
											<li>Implementation when number of layers to be trained exceeds number of active parallel units which can run concurrently in the given environment.</li>
											<li>Scaling the implementation for large datasets.</li>
										</ul>
									</p>
									<h4>Workload Characteristics &amp Constraints</h4>
									<p>
										Every layer might require large input data. Maintaining the outputs of intermediate layers in a constrained memory setting will be a challenge. Optimizing communication between contiguous layers spread across different systems in the distributed environment is the key challenge.
									</p>
								</div>
							</section>

						<!-- Four -->
							<section id="four">
								<div class="container">
									<h3>Resources</h3>
									<p><h4>Reference papers:</h4>
										<ul>
											<li> Santara, Anirban, et al. "Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training." arXiv preprint arXiv:1603.02836 (2016).</li>
											<li> Cui, Henggang, et al. "Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server." Proceedings of the Eleventh European Conference on Computer Systems. ACM, 2016. </li>
											<li> Bengio, Yoshua, et al. "Greedy layer-wise training of deep networks." Advances in neural information processing systems 19 (2007): 153. </li>
										</ul></p>
									<p><h4>Computing Resources:</h4>
										<ul>
											<li> Latedays cluster for development and testing (Distributed multi-core CPU-GPU environment) </li>
										</ul></p>
									<p><h4>Starter code:</h4>
										<ul>
											<li> Considering using an efficient single-GPU implementation of autoencoder training from Caffe, Theano or TensorFlow. </li>
										</ul></p>
								</div>
							</section>

						<!-- Five -->
							<section id="five">
								<div class="container">
									<h3>Goals &amp Deliverables</h3>
									<p><ul>
										<li> WE PLAN to achieve an efficient multi-GPU implementation of synchronized layer-wise pre-training algorithm of deep stacked autoencoders. An existing multi-core implementation of this algorithm has been reported to give a speed-up of over 25%. We plan to improve this by implementing the same in an efficient manner, potentially achieving greater speedup.</li>
										<li> WE HOPE to achieve an efficient distributed implementation and a generalized distributed version of this algorithm and open source the same if the results are promising.</li>
										<li> In the parallelism competition, we will exhibit analysis of speedup, with accuracies achieved on standard benchmark neural networks.</li>
										<li> We plan to analyze and understand performance improvements and accuracy guarantees for benchmark deep neural networks for varying data sizes and number of layers of pre-training.</li>
									</ul></p>
								</div>
							</section>

						<!-- Six -->
							<section id="six">
								<div class="container">
									<h3>Platform</h3>
									<p>The cluster contains 18 machines (1 head node + 17 worker nodes). Each machine features:
										<ul>
											<li> Two, six-core <a href="http://ark.intel.com/products/83352/Intel-Xeon-Processor-E5-2620-v3-15M-Cache-2_40-GHz" target="_blank">Xeon e5-2620 v3 processors </a> (2.4 GHz, 15MB L3 cache, hyper-threading, AVX2 instruction support) </li>
											<li> 16 GB RAM (60 GB/sec of BW) </li>
											<li> 3 machines with 12 GB Titan X GPU and rest with Titan K40 12 GB GPU </li>
										</ul>
								</div>
							</section>

						<!-- Seven -->
							<section id="seven">
								<div class="container">
									<h3>Updated Schedule</h3>
									<p>The list of tasks with tentative completion dates are enlisted below:</p>
									<ul>
										<li> Extensive Literature Review - April 14, 2017 [Completed]</li>
										<li> Analyze and finalize choice of programming platform and install required softwares - April 14, 2017 [Completed]</li>
										<li> Baseline 1: Implement greedy layer-wise pre-training of deep stacked autoencoders in CUDA (Using Tensorflow) - April 14, 2017 [Completed]</li>
										<li> Baseline 2: Implement synchronized layer-wise pre-training of deep stacked autoencoders on single GPU - April 21, 2017 [Completed]</li>
										<li> Implement 2-GPU single node implementation of synchronized layer-wise pre-training of deep stacked autoencoders - April 27, 2017 [Almost Done. Resolving minor issues in the code]</li>
										<li> Implement generalized multi-GPU single node implementation of synchronized layer-wise pre-training of deep stacked autoencoders and analyze various training strategies - May 4, 2017</li>
										<li> Analyze the various training strategies for large datasets and deeper networks and compare performance - May 8, 2017 </li>
										<li> Implement distributed single-GPU multi-node implementation of synchronized layer-wise pre-training of deep stacked autoencoders - May 10, 2017</li>
									</ul>
								</div>
							</section>							

					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>Design: <a href="http://html5up.net">HTML5 UP</a></li><li>Images from <a href="https://blogs-images.forbes.com/kevinmurnane/files/2016/03/google-deepmind-artificial-intelligence-2-970x0-970x646.jpg?width=960" target="_blank">here</a> and <a href="https://assets.wired.com/photos/w_1720/wp-content/uploads/2016/12/GettyImages-627219854.jpg" target="_blank">here</a>.</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
